{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Inference.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "4YrcSSdmZG1a"
      ],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "P2zJ-htv63zI",
        "outputId": "24e32364-95ed-4cc8-bb1a-8285d26cc806"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%cd \"/content/drive/MyDrive/python/cpt-softano/code\""
      ],
      "metadata": {
        "id": "hkVe8aM-gf6B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pytorch_lightning"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "OQD_elWwK0I5",
        "outputId": "58639651-fae9-4d5a-f36a-86c650558aa1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pytorch_lightning\n",
            "  Downloading pytorch_lightning-1.5.9-py3-none-any.whl (527 kB)\n",
            "\u001b[K     |████████████████████████████████| 527 kB 10.3 MB/s \n",
            "\u001b[?25hCollecting fsspec[http]!=2021.06.0,>=2021.05.0\n",
            "  Downloading fsspec-2022.1.0-py3-none-any.whl (133 kB)\n",
            "\u001b[K     |████████████████████████████████| 133 kB 69.4 MB/s \n",
            "\u001b[?25hCollecting setuptools==59.5.0\n",
            "  Downloading setuptools-59.5.0-py3-none-any.whl (952 kB)\n",
            "\u001b[K     |████████████████████████████████| 952 kB 76.8 MB/s \n",
            "\u001b[?25hRequirement already satisfied: tensorboard>=2.2.0 in /usr/local/lib/python3.7/dist-packages (from pytorch_lightning) (2.7.0)\n",
            "Collecting torchmetrics>=0.4.1\n",
            "  Downloading torchmetrics-0.7.0-py3-none-any.whl (396 kB)\n",
            "\u001b[K     |████████████████████████████████| 396 kB 80.2 MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.17.2 in /usr/local/lib/python3.7/dist-packages (from pytorch_lightning) (1.19.5)\n",
            "Requirement already satisfied: tqdm>=4.41.0 in /usr/local/lib/python3.7/dist-packages (from pytorch_lightning) (4.62.3)\n",
            "Requirement already satisfied: torch>=1.7.* in /usr/local/lib/python3.7/dist-packages (from pytorch_lightning) (1.10.0+cu111)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from pytorch_lightning) (3.10.0.2)\n",
            "Collecting future>=0.17.1\n",
            "  Downloading future-0.18.2.tar.gz (829 kB)\n",
            "\u001b[K     |████████████████████████████████| 829 kB 69.5 MB/s \n",
            "\u001b[?25hRequirement already satisfied: packaging>=17.0 in /usr/local/lib/python3.7/dist-packages (from pytorch_lightning) (21.3)\n",
            "Collecting PyYAML>=5.1\n",
            "  Downloading PyYAML-6.0-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (596 kB)\n",
            "\u001b[K     |████████████████████████████████| 596 kB 54.7 MB/s \n",
            "\u001b[?25hCollecting pyDeprecate==0.3.1\n",
            "  Downloading pyDeprecate-0.3.1-py3-none-any.whl (10 kB)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from fsspec[http]!=2021.06.0,>=2021.05.0->pytorch_lightning) (2.23.0)\n",
            "Collecting aiohttp\n",
            "  Downloading aiohttp-3.8.1-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (1.1 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.1 MB 66.6 MB/s \n",
            "\u001b[?25hRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=17.0->pytorch_lightning) (3.0.7)\n",
            "Requirement already satisfied: wheel>=0.26 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.2.0->pytorch_lightning) (0.37.1)\n",
            "Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.2.0->pytorch_lightning) (0.6.1)\n",
            "Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.2.0->pytorch_lightning) (1.35.0)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.2.0->pytorch_lightning) (1.0.1)\n",
            "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.2.0->pytorch_lightning) (1.8.1)\n",
            "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.2.0->pytorch_lightning) (0.4.6)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.2.0->pytorch_lightning) (3.3.6)\n",
            "Requirement already satisfied: grpcio>=1.24.3 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.2.0->pytorch_lightning) (1.43.0)\n",
            "Requirement already satisfied: absl-py>=0.4 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.2.0->pytorch_lightning) (1.0.0)\n",
            "Requirement already satisfied: protobuf>=3.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.2.0->pytorch_lightning) (3.17.3)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from absl-py>=0.4->tensorboard>=2.2.0->pytorch_lightning) (1.15.0)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard>=2.2.0->pytorch_lightning) (4.8)\n",
            "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard>=2.2.0->pytorch_lightning) (4.2.4)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard>=2.2.0->pytorch_lightning) (0.2.8)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.7/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard>=2.2.0->pytorch_lightning) (1.3.0)\n",
            "Requirement already satisfied: importlib-metadata>=4.4 in /usr/local/lib/python3.7/dist-packages (from markdown>=2.6.8->tensorboard>=2.2.0->pytorch_lightning) (4.10.1)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard>=2.2.0->pytorch_lightning) (3.7.0)\n",
            "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.7/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard>=2.2.0->pytorch_lightning) (0.4.8)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->fsspec[http]!=2021.06.0,>=2021.05.0->pytorch_lightning) (2021.10.8)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->fsspec[http]!=2021.06.0,>=2021.05.0->pytorch_lightning) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->fsspec[http]!=2021.06.0,>=2021.05.0->pytorch_lightning) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->fsspec[http]!=2021.06.0,>=2021.05.0->pytorch_lightning) (1.24.3)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard>=2.2.0->pytorch_lightning) (3.1.1)\n",
            "Collecting asynctest==0.13.0\n",
            "  Downloading asynctest-0.13.0-py3-none-any.whl (26 kB)\n",
            "Collecting frozenlist>=1.1.1\n",
            "  Downloading frozenlist-1.3.0-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (144 kB)\n",
            "\u001b[K     |████████████████████████████████| 144 kB 93.9 MB/s \n",
            "\u001b[?25hCollecting async-timeout<5.0,>=4.0.0a3\n",
            "  Downloading async_timeout-4.0.2-py3-none-any.whl (5.8 kB)\n",
            "Collecting yarl<2.0,>=1.0\n",
            "  Downloading yarl-1.7.2-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (271 kB)\n",
            "\u001b[K     |████████████████████████████████| 271 kB 98.7 MB/s \n",
            "\u001b[?25hRequirement already satisfied: charset-normalizer<3.0,>=2.0 in /usr/local/lib/python3.7/dist-packages (from aiohttp->fsspec[http]!=2021.06.0,>=2021.05.0->pytorch_lightning) (2.0.10)\n",
            "Collecting aiosignal>=1.1.2\n",
            "  Downloading aiosignal-1.2.0-py3-none-any.whl (8.2 kB)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.7/dist-packages (from aiohttp->fsspec[http]!=2021.06.0,>=2021.05.0->pytorch_lightning) (21.4.0)\n",
            "Collecting multidict<7.0,>=4.5\n",
            "  Downloading multidict-6.0.2-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (94 kB)\n",
            "\u001b[K     |████████████████████████████████| 94 kB 4.7 MB/s \n",
            "\u001b[?25hBuilding wheels for collected packages: future\n",
            "  Building wheel for future (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for future: filename=future-0.18.2-py3-none-any.whl size=491070 sha256=6eb799f07b8d3c8aea00dfa00faa5b34f3b5e8ce5764fbd6689bdd7ede8e3c3a\n",
            "  Stored in directory: /root/.cache/pip/wheels/56/b0/fe/4410d17b32f1f0c3cf54cdfb2bc04d7b4b8f4ae377e2229ba0\n",
            "Successfully built future\n",
            "Installing collected packages: setuptools, multidict, frozenlist, yarl, asynctest, async-timeout, aiosignal, pyDeprecate, fsspec, aiohttp, torchmetrics, PyYAML, future, pytorch-lightning\n",
            "  Attempting uninstall: setuptools\n",
            "    Found existing installation: setuptools 57.4.0\n",
            "    Uninstalling setuptools-57.4.0:\n",
            "      Successfully uninstalled setuptools-57.4.0\n",
            "  Attempting uninstall: PyYAML\n",
            "    Found existing installation: PyYAML 3.13\n",
            "    Uninstalling PyYAML-3.13:\n",
            "      Successfully uninstalled PyYAML-3.13\n",
            "  Attempting uninstall: future\n",
            "    Found existing installation: future 0.16.0\n",
            "    Uninstalling future-0.16.0:\n",
            "      Successfully uninstalled future-0.16.0\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "datascience 0.10.6 requires folium==0.2.1, but you have folium 0.8.3 which is incompatible.\u001b[0m\n",
            "Successfully installed PyYAML-6.0 aiohttp-3.8.1 aiosignal-1.2.0 async-timeout-4.0.2 asynctest-0.13.0 frozenlist-1.3.0 fsspec-2022.1.0 future-0.18.2 multidict-6.0.2 pyDeprecate-0.3.1 pytorch-lightning-1.5.9 setuptools-59.5.0 torchmetrics-0.7.0 yarl-1.7.2\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "pkg_resources"
                ]
              }
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from pathlib import Path\n",
        "import warnings\n",
        "from pytorch_lightning import Trainer\n",
        "from pytorch_lightning.callbacks import ModelCheckpoint\n",
        "from pytorch_lightning.core.datamodule import LightningDataModule\n",
        "from pytorch_lightning.core.lightning import LightningModule\n",
        "from pytorch_lightning.loggers.csv_logs import CSVLogger\n",
        "\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torch.optim import lr_scheduler\n",
        "import torch.nn as nn\n",
        "import torch\n",
        "from torch.optim import AdamW\n",
        "from sklearn.model_selection import GroupKFold\n",
        "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
        "\n",
        "from multiprocessing import Process, Pool\n",
        "from torch.multiprocessing import Pool, Process, set_start_method\n",
        "\n",
        "import gc\n",
        "from glob import glob\n",
        "\n",
        "import multiprocessing\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import logging\n",
        "\n",
        "import random\n",
        "\n",
        "from tqdm.notebook import tqdm\n",
        "from sklearn import metrics\n",
        "\n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "import datetime\n"
      ],
      "metadata": {
        "id": "H4UVJ_KB8pJ9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test = pd.read_csv(\"/content/drive/MyDrive/python/cpt-softano/data/test.csv\")"
      ],
      "metadata": {
        "id": "EHmem_CLnP_m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def isSeqencial_df(df, retPos=False):\n",
        "    df[\"isAnomaly\"] = df[\"isAnomaly\"].apply(lambda x: 1 if x==True else 0)\n",
        "    df[\"diff\"] = df[\"isAnomaly\"].diff()\n",
        "\n",
        "    risingEdgeNum = (df[\"diff\"]==1).sum()\n",
        "    fallingEdgeNum = (df[\"diff\"]==-1).sum()\n",
        "\n",
        "    if retPos:\n",
        "        risingPos = df[df[\"diff\"]==1].index.tolist()\n",
        "        fallingPos = df[df[\"diff\"]==-1].index.tolist()\n",
        "\n",
        "        return risingEdgeNum, fallingEdgeNum, risingPos, fallingPos\n",
        "    else:\n",
        "        return risingEdgeNum, fallingEdgeNum"
      ],
      "metadata": {
        "id": "70iSGSITD7-l"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "hostwls_pattern = test[[\"timestamp\",'host', 'process']].groupby(['host', 'process']).count().index.tolist()\n",
        "\n",
        "group_dir = \"../output/group_csvs/test/\"\n",
        "if not os.path.exists(group_dir):\n",
        "    os.mkdir(group_dir)\n",
        "\n",
        "    for pattern in hostwls_pattern:\n",
        "\n",
        "        host = pattern[0]\n",
        "        process = pattern[1]\n",
        "        df_tmp = test[(test[\"host\"]==host) & (test[\"process\"]==process)]\n",
        "\n",
        "        time = df_tmp[\"timestamp\"].iloc[0].replace(\" \", \"-\")\n",
        "        df_tmp.to_csv(group_dir + f\"{host}_{process}_{time}.csv\", index=False)\n",
        "\n",
        "    del df_tmp\n",
        "    gc.collect()"
      ],
      "metadata": {
        "id": "97xcnmlnn1kw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "group_dir = \"../output/group_csvs/test/\"\n",
        "\n",
        "group_csvs = glob(group_dir+\"*.csv\")\n",
        "group_csvs"
      ],
      "metadata": {
        "id": "jRkE6CP-gEdX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "CONFIG = {\n",
        "    \"exp_name\" : \"lstm_window15\",\n",
        "    \"seed\": 42,\n",
        "    \"epochs\": 7,    \n",
        "    \"train_batch_size\": 64,    \n",
        "    \"valid_batch_size\": 128,    \n",
        "    \"learning_rate\": 0.0001, \n",
        "    \"scheduler\": 'CosineAnnealingLR',    \n",
        "    \"min_lr\": 0.000001,\n",
        "    \"n_fold\": 5,    \n",
        "    \"num_classes\": 1,    \n",
        "    \"num_workers\": 2,    \n",
        "    \"device\": torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\"),    \n",
        "}"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "reWq24jvvyDb",
        "outputId": "0628ffa1-3c4d-4a6c-dadb-3d1598612432"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "config file ---->  /content/drive/MyDrive/python/cpt-softano/prepare_tutorial/code/config/config.yaml\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "gc.enable()\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(device)\n",
        "\n",
        "SEED = 2022"
      ],
      "metadata": {
        "id": "FQ4d3I_UYOdT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class TimeSeriseDataSet(Dataset):\n",
        "    def __init__(self, data, N_FEAT, featurecols, window, targetcol=None , flag='TRAIN'):\n",
        "        self.data = data\n",
        "        self.n_feat = N_FEAT\n",
        "        self.cols = featurecols\n",
        "        self.target = targetcol\n",
        "        self.flag = flag\n",
        "        self.window = window\n",
        "\n",
        "    def __len__(self):\n",
        "        return self.data.shape[0]\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "\n",
        "        if self.flag == 'TRAIN':\n",
        "            if idx > self.window:\n",
        "                data = self.data[ self.cols ].iloc[idx-self.window: idx].values.astype(np.float32)\n",
        "                y = self.data[ self.target ].iloc[idx].astype(np.float32) \n",
        "            else:\n",
        "                data = self.data[ self.cols ].iloc[idx: idx+self.window].values.astype(np.float32)\n",
        "                y = self.data[ self.target ].iloc[ self.window ].astype(np.float32) \n",
        "            return data, y\n",
        "        else:\n",
        "            if idx > self.window:\n",
        "                data = self.data[ self.cols ].iloc[idx-self.window: idx].values.astype(np.float32)\n",
        "                data = torch.from_numpy(data)\n",
        "                data = data.to(device)                \n",
        "            else:\n",
        "                data = self.data[ self.cols ].iloc[0: 0+self.window].values.astype(np.float32) \n",
        "                data = torch.from_numpy(data)\n",
        "                data = data.to(device) \n",
        "            return data\n",
        "\n",
        "class TimeSeriseDataModule(LightningDataModule):\n",
        "\n",
        "    def __init__(self, data=None, test_data=None, train_id=None, valid_id=None, feature_cols=None ,target_col=None ,window=None, N_FEAT=None, BS=None):\n",
        "        super().__init__()\n",
        "\n",
        "        self.data = data\n",
        "        self.test_data = test_data\n",
        "        self.train_id = train_id\n",
        "        self.valid_id = valid_id\n",
        "        self.feature_cols = feature_cols\n",
        "        self.target_col = target_col\n",
        "        self.window = window\n",
        "\n",
        "        self.BS = BS\n",
        "\n",
        "        self.N_FEAT = N_FEAT\n",
        "\n",
        "    def train_dataloader(self):\n",
        "        train_ds = TimeSeriseDataSet(data=self.data.iloc[self.train_id,:], N_FEAT=self.N_FEAT, featurecols=self.feature_cols, targetcol=self.target_col, window=self.window,flag='TRAIN')\n",
        "        train_dl = DataLoader(train_ds, batch_size=self.BS,\n",
        "                              shuffle=True, drop_last=False )\n",
        "        return train_dl\n",
        "\n",
        "    def val_dataloader(self):\n",
        "        valid_ds = TimeSeriseDataSet(data=self.data.iloc[self.valid_id,:], N_FEAT=self.N_FEAT, featurecols=self.feature_cols, targetcol=self.target_col, window=self.window,flag='TRAIN')\n",
        "        valid_dl = DataLoader(valid_ds, batch_size=self.BS,\n",
        "                              shuffle=False, drop_last=False)\n",
        "        return valid_dl\n",
        "\n",
        "    def test_dataloader(self):\n",
        "        test_ds = TimeSeriseDataSet(data=self.test_data, N_FEAT=self.N_FEAT, featurecols=self.feature_cols, flag='TEST', window=self.window)\n",
        "        test_dl = DataLoader(test_ds, batch_size=self.BS,\n",
        "                             shuffle=False)\n",
        "        return test_dl\n"
      ],
      "metadata": {
        "id": "QvlxHcUzGmJi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def fetch_scheduler(optimizer):\n",
        "    \n",
        "    if CONFIG['scheduler'] == 'CosineAnnealingLR':\n",
        "        scheduler = lr_scheduler.CosineAnnealingLR(\n",
        "            optimizer,\n",
        "            T_max=CONFIG['T_max'],\n",
        "            eta_min=CONFIG['min_lr']\n",
        "        )\n",
        "        \n",
        "    elif CONFIG['scheduler'] == 'CosineAnnealingWarmRestarts':\n",
        "        scheduler = lr_scheduler.CosineAnnealingWarmRestarts(\n",
        "            optimizer,\n",
        "            T_0=CONFIG['T_0'],\n",
        "            eta_min=CONFIG['min_lr']\n",
        "        )\n",
        "        \n",
        "    elif CONFIG['scheduler'] == None:\n",
        "        return None\n",
        "        \n",
        "    return scheduler"
      ],
      "metadata": {
        "id": "W0OR2zJ_vbF3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class RNNModel(LightningModule):\n",
        "    def __init__(\n",
        "        self,\n",
        "        input_dim=16, \n",
        "        lstm_dim=256,\n",
        "        dense_dim=256,\n",
        "        logit_dim=256,\n",
        "        num_classes=1,\n",
        "    ):\n",
        "        super().__init__()\n",
        "\n",
        "        self.mlp = nn.Sequential(\n",
        "            nn.Linear(input_dim, dense_dim // 2),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(dense_dim // 2, dense_dim),\n",
        "            nn.ReLU(),\n",
        "        )\n",
        "\n",
        "        self.lstm = nn.LSTM(dense_dim, lstm_dim, batch_first=True, bidirectional=True)\n",
        "\n",
        "        self.logits = nn.Sequential(\n",
        "            nn.Linear(lstm_dim * 2, logit_dim),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(logit_dim, num_classes),\n",
        "        )\n",
        "        self.m = nn.Sigmoid()\n",
        "\n",
        "    def forward(self, x):\n",
        "        features = self.mlp(x)\n",
        "        features, _ = self.lstm(features)\n",
        "        pred = self.logits(features[:,-1])\n",
        "        pred = self.m(pred)\n",
        "        #return pred\n",
        "        return pred[:,-1]\n",
        "\n",
        "    def training_step(self, batch, batch_idx):\n",
        "        \n",
        "        x, targets = batch\n",
        "        outputs = self(x)\n",
        "        loss = self.criterion(outputs, targets)\n",
        "        self.log(\"train_loss\", loss, prog_bar=True, logger=True)\n",
        "\n",
        "        return {\"loss\": loss}\n",
        "    \n",
        "    def validation_step(self, batch, batch_idx):\n",
        "        x, targets = batch\n",
        "\n",
        "        outputs = self(x)\n",
        "        loss = self.criterion(outputs, targets)\n",
        "        self.log(\"val_loss\", loss, prog_bar=True, logger=True)\n",
        "        \n",
        "        return {'val_loss': loss}\n",
        "        \n",
        "    def configure_optimizers(self):\n",
        "        \n",
        "        optimizer = AdamW(self.parameters())\n",
        "        scheduler = fetch_scheduler(optimizer)\n",
        "        \n",
        "        return dict(\n",
        "            optimizer = optimizer,\n",
        "            lr_scheduler = scheduler\n",
        "        )\n",
        "\n",
        "    def criterion(self, outputs, targets):\n",
        "        loss = nn.BCELoss()\n",
        "        return loss(outputs, targets)"
      ],
      "metadata": {
        "id": "g3wK6TDifiLZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def preprocess_df(df_, df_test=None, N_FEAT=None, feature_cols=None):\n",
        "\n",
        "    gc.collect()\n",
        "\n",
        "    ss = StandardScaler()\n",
        "    _ = ss.fit(df_.loc[:, feature_cols])\n",
        "\n",
        "    data = df_.copy()\n",
        "\n",
        "    data.loc[:, feature_cols] = ss.transform(data.loc[:, feature_cols])\n",
        "\n",
        "    if df_test:\n",
        "        test_data = df_test.copy()\n",
        "        test_data.loc[:,feature_cols] = ss.transform(test_data.loc[:, feature_cols])\n",
        "\n",
        "        return data, test_data\n",
        "    \n",
        "    return data\n"
      ],
      "metadata": {
        "id": "viGHiZKGKvhw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "@torch.no_grad()\n",
        "def inference(model, dataloader, device):\n",
        "    model.eval()\n",
        "    \n",
        "    dataset_size = 0\n",
        "    running_loss = 0.0\n",
        "    \n",
        "    PREDS = []\n",
        "    \n",
        "    bar = tqdm(enumerate(dataloader), total=len(dataloader))\n",
        "\n",
        "    for step, data in bar:\n",
        "        \n",
        "        outputs = model(data)\n",
        "        PREDS.append(outputs.view(-1).cpu().detach().numpy()) \n",
        "    \n",
        "    PREDS = np.concatenate(PREDS)\n",
        "    gc.collect()\n",
        "    \n",
        "    return PREDS"
      ],
      "metadata": {
        "id": "vb4NzHZ8YqqO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "feature_cols = ['Stuck threads : (MXBean(com.bea:Name=ThreadPoolRuntime,Type=ThreadPoolRuntime).StuckThreadCount)',\n",
        "    'Connection delay : (MXBean(com.bea:Name=source05,Type=JDBCConnectionPoolRuntime).ConnectionDelayTime)',\n",
        "    'Connection delay : (MXBean(com.bea:Name=source04,Type=JDBCDataSourceRuntime).ConnectionDelayTime)',\n",
        "    'Connection delay : (MXBean(com.bea:Name=source02,Type=JDBCDataSourceRuntime).ConnectionDelayTime)',\n",
        "    'Connection delay : (MXBean(com.bea:Name=source03,Type=JDBCConnectionPoolRuntime).ConnectionDelayTime)',\n",
        "    'Connection delay : (MXBean(com.bea:Name=source01,Type=JDBCDataSourceRuntime).ConnectionDelayTime)',\n",
        "    'Active connections : (MXBean(com.bea:Name=source09,Type=JDBCDataSourceRuntime).ActiveConnectionsCurrentCount)',\n",
        "    'Rel. physical mem usage : ((MXBean(java.lang:type=OperatingSystem).FreePhysicalMemorySize / MXBean(java.lang:type=OperatingSystem).TotalPhysicalMemorySize))',\n",
        "    'Rel. swap usage : ((MXBean(java.lang:type=OperatingSystem).FreeSwapSpaceSize / MXBean(java.lang:type=OperatingSystem).TotalSwapSpaceSize))',\n",
        "    'Connection delay : (MXBean(com.bea:Name=source09,Type=JDBCDataSourceRuntime).ConnectionDelayTime)',\n",
        "    'Memory space usage : ((MXBean(java.lang:name=Code Cache,type=MemoryPool).Usage.committed / MXBean(java.lang:name=Code Cache,type=MemoryPool).Usage.max))',\n",
        "    'Rel. open file descriptors : ((MXBean(java.lang:type=OperatingSystem).OpenFileDescriptorCount / MXBean(java.lang:type=OperatingSystem).MaxFileDescriptorCount))',\n",
        "    'Memory space usage : ((MXBean(java.lang:name=PS Perm Gen,type=MemoryPool).Usage.committed / MXBean(java.lang:name=PS Perm Gen,type=MemoryPool).Usage.max))',\n",
        "    'Reserve request activity : (incld/dx (MXBean(com.bea:Name=source09,Type=JDBCDataSourceRuntime).ReserveRequestCount))',\n",
        "    'Memory space usage : ((MXBean(java.lang:name=PS Perm Gen,type=MemoryPool).Usage.used / MXBean(java.lang:name=PS Perm Gen,type=MemoryPool).Usage.max))',\n",
        "    'Prepared statement cache hit rate : ((MXBean(com.bea:Name=source09,Type=JDBCDataSourceRuntime).PrepStmtCacheHitCount / MXBean(com.bea:Name=source09,Type=JDBCDataSourceRuntime).PrepStmtCacheMissCount))',\n",
        "]"
      ],
      "metadata": {
        "id": "HJvcGkvBvka4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "window=15\n",
        "\n",
        "BS = CONFIG[\"valid_batch_size\"]\n",
        "N_FEAT = len(feature_cols)\n",
        "\n",
        "modelpath = glob(\"/content/drive/MyDrive/python/cpt-softano/models/LSTM/checkpoints/2022-02-01-04-43/*.ckpt\")\n",
        "\n",
        "model = RNNModel( input_dim=N_FEAT )\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "model = model.load_from_checkpoint(checkpoint_path=modelpath[-1])\n",
        "model.to(device)\n",
        "\n",
        "for targetcsv in group_csvs:\n",
        "    test = pd.read_csv(targetcsv)\n",
        "    test_df = test[feature_cols]\n",
        "\n",
        "    data = preprocess_df(test_df, N_FEAT=N_FEAT, feature_cols=feature_cols)\n",
        "\n",
        "    dm = TimeSeriseDataModule(test_data=data, feature_cols=feature_cols, window=window, N_FEAT=N_FEAT, BS=BS)\n",
        "    test_loader = dm.test_dataloader()\n",
        "\n",
        "    pred = inference(model, test_loader, device)\n",
        "\n",
        "    f_name = \"../output/submission/sub_\" + target_csv.split(\"/\")[-1] \n",
        "    test[\"Anomaly\"] = pred\n",
        "\n",
        "    test[[\"id\",\"Anomaly\"]].to_csv(f_name, index=False)\n"
      ],
      "metadata": {
        "id": "fgmbTrWEw7j7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## EOF"
      ],
      "metadata": {
        "id": "4YrcSSdmZG1a"
      }
    }
  ]
}